{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6657e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f1fd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the good stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from smallscout.params import PREPROCESSOR_PATH\n",
    "#from datetime import datetime\n",
    "#import os\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3b89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the preprocessor\n",
    "#PREPROCESSOR_FILE_PATH = \"../Models/logistic_regression_preprocessor.pkl\"\n",
    "\n",
    "# Step 1 : target creation + train_test_split\n",
    "# Creating target variables to automate creation of quarterly, yearly and 2-yearly targets, because well, DON'T REPEAT YOURSELF!\n",
    "def create_target_variable(df, frequency:int, threshold):\n",
    "    if frequency == 1:\n",
    "        col = 'mc_qtr_growth_pct'\n",
    "    if frequency == 4:\n",
    "        col = 'mc_yr_growth_pct'\n",
    "    if frequency == 8:\n",
    "        col = 'mc_2yr_growth_pct'\n",
    "   #else:\n",
    "   #    raise ValueError(\"Invalid frequency. Use 1 (quarterly), 4 (yearly), or 8 (2-year).\")\n",
    "    df[col] = df[col].shift(-frequency)\n",
    "    df.dropna(subset=col, inplace=True)\n",
    "    target_func = lambda x: 1 if ((x[col] > threshold) & (x.small_cap == 1)) else 0\n",
    "    df['target'] = df.apply(target_func, axis=1)\n",
    "    return df\n",
    "\n",
    "def drop_columns(df, cols_to_drop=None):\n",
    "    \"\"\"Drops specified columns from the DataFrame.\"\"\"\n",
    "    if cols_to_drop is None:\n",
    "        # Default columns to drop if none are specified\n",
    "        cols_to_drop = ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year']\n",
    "    return df.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "\n",
    "# Creating a custom function for the group split\n",
    "def group_train_test_split(data, test_size=0.2, random_state=None):\n",
    "    # We split by groups (company ticker) while keeping the data structure intact.\n",
    "    unique_groups = data['TICKER'].unique()\n",
    "    train_groups, test_groups = train_test_split(unique_groups, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train = data[data['TICKER'].isin(train_groups)]\n",
    "    X_test = data[data['TICKER'].isin(test_groups)]\n",
    "\n",
    "    # Define columns to drop: Ticker, cik, date, quarter, year + growth columns\n",
    "    cols_to_drop = ['mc_qtr_growth', 'mc_qtr_growth_pct', 'mc_yr_growth', 'mc_yr_growth_pct', 'mc_2yr_growth', 'mc_2yr_growth_pct']\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    X_train = drop_columns(X_train, cols_to_drop + ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year'])\n",
    "    X_test = drop_columns(X_test, cols_to_drop + ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year'])\n",
    "\n",
    "    # Extract the target variable from the dataset\n",
    "    y_train = data[data['TICKER'].isin(train_groups)]['target']\n",
    "    y_test = data[data['TICKER'].isin(test_groups)]['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 2: Identify numerical and categorical features\n",
    "def identify_feature_types(df):\n",
    "    \"\"\"Identifies the numerical and categorical columns in the DataFrame.\"\"\"\n",
    "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Exclude 'Ticker' from categorical features as it's not needed for transformation\n",
    "    if 'TICKER' in categorical_features:\n",
    "        categorical_features.remove('TICKER')\n",
    "\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "# Step 3: Create preprocessing pipeline for numerical and categorical features\n",
    "def create_preprocessing_pipeline(numerical_features, categorical_features):\n",
    "    \"\"\"Creates the preprocessing pipeline for numerical and categorical features.\"\"\"\n",
    "    # Preprocessing for numerical data: RobustScaler to make our numbers m√°s robusto.\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),  # Handle NaNs\n",
    "        ('scaler', RobustScaler())  # Scale the data\n",
    "    ])\n",
    "\n",
    "    # Preprocessing for categorical data: OneHotEncoder to give each category its own columm...\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categories\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Encode categories\n",
    "    ])\n",
    "\n",
    "    # Combine the transformers into one big ColumnTransformer.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough'  # Columns not specified in 'num' or 'cat' will be passed through unmodified\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Function to save the preprocessor\n",
    "def save_preprocessor(preprocessor, file_path='Models/preprocessor_cross_section.pkl'):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(preprocessor, file)\n",
    "    print(f\"Preprocessor saved to {file_path}\")\n",
    "\n",
    "# Load Pipeline from pickle file\n",
    "#my_pipeline = pickle.load(open(\"pipeline.pkl\",\"rb\"))\n",
    "\n",
    "# Step 4: Function to preprocess data in training mode (fitting the pipeline)\n",
    "def preprocess_training_data(X_train, preprocessor=None):\n",
    "    \"\"\"Fits and transforms the training data using the provided pipeline.\"\"\"\n",
    "    if preprocessor is None:\n",
    "        # Identify feature types\n",
    "        numerical_features, categorical_features = identify_feature_types(X_train)\n",
    "        preprocessor = create_preprocessing_pipeline(numerical_features, categorical_features)\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "    # Save the preprocessor after fitting\n",
    "    save_preprocessor(preprocessor)\n",
    "\n",
    "    return X_train_processed, preprocessor\n",
    "\n",
    "# Step 5: Function to preprocess new/unseen/test data in production mode (only transforming)\n",
    "def preprocess_new_data(X_new, preprocessor):\n",
    "    \"\"\"Transforms new/unseen/test data using a pre-fitted pipeline.\"\"\"\n",
    "    if preprocessor is None:\n",
    "        raise ValueError(\"The preprocessor must be fitted on training data first before transforming new data.\")\n",
    "\n",
    "    # Transform the new data (no fitting here)\n",
    "    X_new_processed = preprocessor.transform(X_new)\n",
    "    return X_new_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be50b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_data/data_for_preprocessing.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5959c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = create_target_variable(df, 4, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed940e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "step2 = drop_columns(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ed67a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = group_train_test_split(step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5cf9f4f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 132377 entries, 0 to 170119\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                                           Non-Null Count   Dtype  \n",
      "---  ------                                                           --------------   -----  \n",
      " 0   Assets                                                           132377 non-null  float64\n",
      " 1   AssetsCurrent                                                    132377 non-null  float64\n",
      " 2   Cash                                                             132377 non-null  float64\n",
      " 3   AssetsNoncurrent                                                 132377 non-null  float64\n",
      " 4   Liabilities                                                      132377 non-null  float64\n",
      " 5   LiabilitiesCurrent                                               132377 non-null  float64\n",
      " 6   LiabilitiesNoncurrent                                            132377 non-null  float64\n",
      " 7   Equity                                                           132377 non-null  float64\n",
      " 8   HolderEquity                                                     132377 non-null  float64\n",
      " 9   RetainedEarnings                                                 132377 non-null  float64\n",
      " 10  AdditionalPaidInCapital                                          132377 non-null  float64\n",
      " 11  TreasuryStockValue                                               132377 non-null  float64\n",
      " 12  TemporaryEquity                                                  132377 non-null  float64\n",
      " 13  RedeemableEquity                                                 132377 non-null  float64\n",
      " 14  LiabilitiesAndEquity                                             132377 non-null  float64\n",
      " 15  Revenues                                                         132377 non-null  float64\n",
      " 16  CostOfRevenue                                                    132377 non-null  float64\n",
      " 17  GrossProfit                                                      132377 non-null  float64\n",
      " 18  OperatingExpenses                                                132377 non-null  float64\n",
      " 19  OperatingIncomeLoss                                              132377 non-null  float64\n",
      " 20  IncomeLossFromContinuingOperationsBeforeIncomeTaxExpenseBenefit  132377 non-null  float64\n",
      " 21  AllIncomeTaxExpenseBenefit                                       132377 non-null  float64\n",
      " 22  IncomeLossFromContinuingOperations                               132377 non-null  float64\n",
      " 23  IncomeLossFromDiscontinuedOperationsNetOfTax                     132377 non-null  float64\n",
      " 24  ProfitLoss                                                       132377 non-null  float64\n",
      " 25  NetIncomeLossAttributableToNoncontrollingInterest                132377 non-null  float64\n",
      " 26  NetIncomeLoss                                                    132377 non-null  float64\n",
      " 27  NetCashProvidedByUsedInOperatingActivitiesContinuingOperations   132377 non-null  float64\n",
      " 28  NetCashProvidedByUsedInFinancingActivitiesContinuingOperations   132377 non-null  float64\n",
      " 29  NetCashProvidedByUsedInInvestingActivitiesContinuingOperations   132377 non-null  float64\n",
      " 30  NetCashProvidedByUsedInOperatingActivities                       132377 non-null  float64\n",
      " 31  NetCashProvidedByUsedInFinancingActivities                       132377 non-null  float64\n",
      " 32  NetCashProvidedByUsedInInvestingActivities                       132377 non-null  float64\n",
      " 33  CashProvidedByUsedInOperatingActivitiesDiscontinuedOperations    132377 non-null  float64\n",
      " 34  CashProvidedByUsedInInvestingActivitiesDiscontinuedOperations    132377 non-null  float64\n",
      " 35  CashProvidedByUsedInFinancingActivitiesDiscontinuedOperations    132377 non-null  float64\n",
      " 36  EffectOfExchangeRateFinal                                        132377 non-null  float64\n",
      " 37  CashPeriodIncreaseDecreaseIncludingExRateEffectFinal             132377 non-null  float64\n",
      " 38  afs                                                              123107 non-null  object \n",
      " 39  sic_2d                                                           132377 non-null  object \n",
      " 40  GDP                                                              132377 non-null  float64\n",
      " 41  interest_rate                                                    132377 non-null  float64\n",
      " 42  unemployment_rate                                                132377 non-null  float64\n",
      " 43  median_cpi                                                       132377 non-null  float64\n",
      " 44  small_cap                                                        132377 non-null  int64  \n",
      " 45  micro_cap                                                        132377 non-null  int64  \n",
      "dtypes: float64(42), int64(2), object(2)\n",
      "memory usage: 47.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#should not have the target, the ticker or the marketcap\n",
    "X_train.drop(['TICKER', 'target', 'market_cap'],inplace=True, axis=1)\n",
    "X_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "880c0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num, cat = identify_feature_types(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "733bb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = create_preprocessing_pipeline(num, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "745e5002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved to Models/preprocessor_cross_section.pkl\n"
     ]
    }
   ],
   "source": [
    "X_train_processed, preprocessor = preprocess_training_data(X_train, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9262078",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Models/preprocessor_2024-09-11_00-26-01.pkl', 'rb') as f_preprocessor:\n",
    "    prep_eoin = pickle.load(f_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cbae6e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'market_cap'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/utils/_indexing.py:361\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 361\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'market_cap'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_processed, preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprep_eoin\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 107\u001b[0m, in \u001b[0;36mpreprocess_training_data\u001b[0;34m(X_train, preprocessor)\u001b[0m\n\u001b[1;32m    104\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m create_preprocessing_pipeline(numerical_features, categorical_features)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Fit and transform the training data\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m X_train_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Save the preprocessor after fitting\u001b[39;00m\n\u001b[1;32m    110\u001b[0m save_preprocessor(preprocessor)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:968\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m    966\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:536\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    534\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    535\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 536\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/small-cap-scout/lib/python3.10/site-packages/sklearn/utils/_indexing.py:369\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    366\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "X_train_processed, preprocessor = preprocess_training_data(X_train, preprocessor=prep_eoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85b4ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_and_save(X_train, y_train, model_dir='Models/'):\n",
    "    \"\"\"Trains, evaluates a logistic regression model, saves the trained model, and returns evaluation metrics.\"\"\"\n",
    "\n",
    "    model_type = 'logistic_regression'\n",
    "    model = LogisticRegression(C=0.001, max_iter=2000, solver='lbfgs')\n",
    "\n",
    "    # Train model with a progress bar\n",
    "    with tqdm(total=100, desc=f\"Training {model_type}\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
    "        model.fit(X_train, y_train)\n",
    "        pbar.update(100)\n",
    "\n",
    "    # Check number of iterations\n",
    "    print(f\"Number of iterations: {model.n_iter_}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    #metrics = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Save the model\n",
    "    #save_model(model, model_type, model_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, model_type, target_horizon, model_dir='~/models/'):\n",
    "    \"\"\"Saves the trained model with a timestamp and prediction target.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    model_filename = f'{model_type}_{target_horizon}_{timestamp}.pkl'\n",
    "\n",
    "    # Ensure model directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "    with open(model_path, 'wb') as f_model:\n",
    "        pickle.dump(model, f_model)\n",
    "\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e350c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "627cb512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training logistic_regression: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà [elapsed: 00:09 left: 00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: [352]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_logistic_regression_and_save(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36df41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor saved to Models/logistic_regression_scTrue_year-ahead_50%.pkl\n"
     ]
    }
   ],
   "source": [
    "save_preprocessor(model, 'Models/logistic_regression_scTrue_year-ahead_50%.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa707ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09aadf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if threshold not in [0.3, 0.5]:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98f2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
