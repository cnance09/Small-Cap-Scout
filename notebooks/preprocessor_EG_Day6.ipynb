{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "vnTExAADftvB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxXym19bhG7A"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "KiMXAWmtprGN"
      },
      "outputs": [],
      "source": [
        "# Step 0 : filter on small caps and micro caps\n",
        "#def filter_data(df, small_cap_value=1, micro_cap_value=1):\n",
        "#    \"\"\"Filters rows based on small_cap and micro_cap values and returns a copy of the filtered DataFrame.\"\"\"\n",
        "#    filtered_df = df[(df['small_cap'] == small_cap_value) & (df['micro_cap'] == micro_cap_value)].copy()\n",
        "#    return filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "FKV_8oKrgACR"
      },
      "outputs": [],
      "source": [
        "# Step 1 : target creation + train_test_split\n",
        "# Creating target variables to automate creation of quarterly, yearly and 2-yearly targets, because well, DON'T REPEAT YOURSELF!\n",
        "def create_target_variable(df, frequency:int, threshold):\n",
        "    if frequency == 1:\n",
        "        col = 'mc_qtr_growth_pct'\n",
        "    if frequency == 4:\n",
        "        col = 'mc_yr_growth_pct'\n",
        "    if frequency == 8:\n",
        "        col = 'mc_2yr_growth_pct'\n",
        "   #else:\n",
        "   #    raise ValueError(\"Invalid frequency. Use 1 (quarterly), 4 (yearly), or 8 (2-year).\")\n",
        "    df[col] = df[col].shift(-frequency)\n",
        "    df.dropna(subset=col, inplace=True)\n",
        "    target_func = lambda x: 1 if ((x[col] > threshold) & (x.small_cap == 1)) else 0\n",
        "    df['target'] = df.apply(target_func, axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "-PdzMchGgMki"
      },
      "outputs": [],
      "source": [
        "def drop_columns(df, cols_to_drop=None):\n",
        "    \"\"\"Drops specified columns from the DataFrame.\"\"\"\n",
        "    if cols_to_drop is None:\n",
        "        # Default columns to drop if none are specified\n",
        "        cols_to_drop = ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year']\n",
        "    return df.drop(cols_to_drop, axis=1, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "71DQCJUxgOaH"
      },
      "outputs": [],
      "source": [
        "# Creating a custom function for the group split\n",
        "def group_train_test_split(data, test_size=0.2, random_state=None):\n",
        "    # We split by groups (company ticker) while keeping the data structure intact.\n",
        "    unique_groups = data['TICKER'].unique()\n",
        "    train_groups, test_groups = train_test_split(unique_groups, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train = data[data['TICKER'].isin(train_groups)]\n",
        "    X_test = data[data['TICKER'].isin(test_groups)]\n",
        "\n",
        "    # Define columns to drop: Ticker, cik, date, quarter, year + growth columns\n",
        "    cols_to_drop = ['mc_qtr_growth', 'mc_qtr_growth_pct', 'mc_yr_growth', 'mc_yr_growth_pct', 'mc_2yr_growth', 'mc_2yr_growth_pct']\n",
        "\n",
        "    # Drop unwanted columns\n",
        "    X_train = drop_columns(X_train, cols_to_drop + ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year'])\n",
        "    X_test = drop_columns(X_test, cols_to_drop + ['cik', 'CIK', 'date', 'stprba', 'quarter', 'year'])\n",
        "\n",
        "    # Extract the target variable from the dataset\n",
        "    y_train = data[data['TICKER'].isin(train_groups)]['target']\n",
        "    y_test = data[data['TICKER'].isin(test_groups)]['target']\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "wvK02xu4gT3D"
      },
      "outputs": [],
      "source": [
        "# Step 2: Identify numerical and categorical features\n",
        "def identify_feature_types(df):\n",
        "    \"\"\"Identifies the numerical and categorical columns in the DataFrame.\"\"\"\n",
        "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Exclude 'Ticker' from categorical features as it's not needed for transformation\n",
        "    if 'TICKER' in categorical_features:\n",
        "        categorical_features.remove('TICKER')\n",
        "\n",
        "    return numerical_features, categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "1TCupWcbgUoZ"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create preprocessing pipeline for numerical and categorical features\n",
        "def create_preprocessing_pipeline(numerical_features, categorical_features):\n",
        "    \"\"\"Creates the preprocessing pipeline for numerical and categorical features.\"\"\"\n",
        "    # Preprocessing for numerical data: RobustScaler to make our numbers más robusto.\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),  # Handle NaNs\n",
        "        ('scaler', RobustScaler())  # Scale the data\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for categorical data: OneHotEncoder to give each category its own columm...\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categories\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Encode categories\n",
        "    ])\n",
        "\n",
        "    # Combine the transformers into one big ColumnTransformer.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "o7c2Bq-wgX2j"
      },
      "outputs": [],
      "source": [
        "# Step 4: Function to preprocess data in training mode (fitting the pipeline)\n",
        "def preprocess_training_data(X_train, preprocessor=None):\n",
        "    \"\"\"Fits and transforms the training data using the provided pipeline.\"\"\"\n",
        "    if preprocessor is None:\n",
        "        # Identify feature types\n",
        "        numerical_features, categorical_features = identify_feature_types(X_train)\n",
        "        preprocessor = create_preprocessing_pipeline(numerical_features, categorical_features)\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "    return X_train_processed, preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "nKfwqkKtglqk"
      },
      "outputs": [],
      "source": [
        "# Step 5: Function to preprocess new/unseen/test data in production mode (only transforming)\n",
        "def preprocess_new_data(X_new, preprocessor):\n",
        "    \"\"\"Transforms new/unseen/test data using a pre-fitted pipeline.\"\"\"\n",
        "    if preprocessor is None:\n",
        "        raise ValueError(\"The preprocessor must be fitted on training data first before transforming new data.\")\n",
        "\n",
        "    # Transform the new data (no fitting here)\n",
        "    X_new_processed = preprocessor.transform(X_new)\n",
        "    return X_new_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43XuFL6dg6GU"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "_avhOzqdgtaj"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_type, model_dir='~/models/'):\n",
        "    \"\"\"Saves the trained model with a timestamp.\"\"\"\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    model_filename = f'{model_type}_{timestamp}.pkl'\n",
        "\n",
        "    # Ensure model directory exists\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = os.path.join(model_dir, model_filename)\n",
        "    with open(model_path, 'wb') as f_model:\n",
        "        pickle.dump(model, f_model)\n",
        "\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "    return model_path\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, scoring_metrics=['accuracy', 'precision', 'recall', 'f1']):\n",
        "    \"\"\"Evaluates the model with cross-validation and test set metrics.\"\"\"\n",
        "    cv_metrics = {}\n",
        "    for metric in scoring_metrics:\n",
        "        with tqdm(total=5, desc=f\"Cross-Validation ({metric})\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "            cv_metrics[metric] = cross_val_score(model, X_train, y_train, cv=5, scoring=metric)\n",
        "            pbar.update(5)\n",
        "\n",
        "    print(f\"Cross-validated Metrics: {', '.join([f'{m}: {cv_metrics[m].mean():.4f}' for m in cv_metrics])}\")\n",
        "\n",
        "    # Test on the test set\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Calculate test set metrics\n",
        "    test_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred_test),\n",
        "        'precision': precision_score(y_test, y_pred_test),\n",
        "        'recall': recall_score(y_test, y_pred_test),\n",
        "        'f1': f1_score(y_test, y_pred_test)\n",
        "    }\n",
        "\n",
        "    # Combine cross-validated and test metrics\n",
        "    metrics = {**{f'cv_{m}': cv_metrics[m].mean() for m in cv_metrics}, **test_metrics}\n",
        "    return metrics\n",
        "\n",
        "def train_logistic_regression_and_save(X_train, y_train, X_test, y_test, model_dir='~/models/'):\n",
        "    \"\"\"Trains, evaluates a logistic regression model, saves the trained model, and returns evaluation metrics.\"\"\"\n",
        "\n",
        "    model_type = 'logistic_regression'\n",
        "    model = LogisticRegression(C=0.001, max_iter=2000, solver='lbfgs')\n",
        "\n",
        "    # Train model with a progress bar\n",
        "    with tqdm(total=100, desc=f\"Training {model_type}\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "        model.fit(X_train, y_train)\n",
        "        pbar.update(100)\n",
        "\n",
        "    # Check number of iterations\n",
        "    print(f\"Number of iterations: {model.n_iter_}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Save the model\n",
        "    save_model(model, model_type, model_dir)\n",
        "\n",
        "    return metrics, model\n",
        "\n",
        "def train_knn_and_save(X_train, y_train, X_test, y_test, model_dir='~/models/'):\n",
        "    \"\"\"Trains, evaluates a K-Nearest Neighbors model, saves the trained model, and returns evaluation metrics.\"\"\"\n",
        "\n",
        "    model_type = 'knn'\n",
        "    knn = KNeighborsClassifier()\n",
        "\n",
        "    # Train model with a progress bar\n",
        "    with tqdm(total=100, desc=f\"Training {model_type}\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "        knn.fit(X_train, y_train)\n",
        "        pbar.update(100)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(knn, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Save the model\n",
        "    save_model(knn, model_type, model_dir)\n",
        "\n",
        "    return metrics, knn\n",
        "\n",
        "def train_svc_rbf_and_save(X_train, y_train, X_test, y_test, model_dir='~/models/'):\n",
        "    \"\"\"Trains, evaluates an SVM with RBF kernel, saves the trained model, and returns evaluation metrics.\"\"\"\n",
        "\n",
        "    model_type = 'svc_rbf'\n",
        "    svc_rbf = SVC(kernel='rbf', probability=True)  # Set `probability=True` for log_loss and cross-validation\n",
        "\n",
        "    # Train model with a progress bar\n",
        "    with tqdm(total=100, desc=f\"Training {model_type}\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "        svc_rbf.fit(X_train, y_train)\n",
        "        pbar.update(100)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(svc_rbf, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Save the model\n",
        "    save_model(svc_rbf, model_type, model_dir)\n",
        "\n",
        "    return metrics, svc_rbf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Step 6: Function to train logistic regression, evaluate, and save the model\\ndef train_logistic_regression_and_save(X_train, y_train, X_test, y_test, model_dir=\\'~/models/\\'):\\n    \"\"\"Trains, evaluates a logistic regression model, saves the trained model with a timestamp and returns evaluation metrics.\"\"\"\\n\\n    # Step 1: Train logistic regression model with a progress bar\\n    logistic_model = LogisticRegression(solver=\\'saga\\', max_iter=2000)\\n    with tqdm(total=100, desc=\"Training Logistic Regression\", bar_format=\\'{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]\\') as pbar:\\n        logistic_model.fit(X_train, y_train)\\n        pbar.update(100)\\n\\n    # Check number of iterations\\n    print(f\"Number of iterations: {logistic_model.n_iter_}\")\\n\\n    # Step 2: Evaluate using cross-validation for accuracy, precision, recall, and F1-score\\n    cv_metrics = {}\\n    for metric in [\\'accuracy\\', \\'precision\\', \\'recall\\', \\'f1\\']:\\n        with tqdm(total=5, desc=f\"Cross-Validation ({metric})\", bar_format=\\'{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]\\') as pbar:\\n            cv_metrics[metric] = cross_val_score(logistic_model, X_train, y_train, cv=5, scoring=metric)\\n            pbar.update(5)\\n\\n    print(f\"Cross-validated Metrics: {\\', \\'.join([f\\'{m}: {cv_metrics[m].mean():.4f}\\' for m in cv_metrics])}\")\\n\\n    # Step 3: Test on the test set\\n    y_pred_test = logistic_model.predict(X_test)\\n\\n    # Calculate test set metrics\\n    test_metrics = {\\n        \\'accuracy\\': accuracy_score(y_test, y_pred_test),\\n        \\'precision\\': precision_score(y_test, y_pred_test),\\n        \\'recall\\': recall_score(y_test, y_pred_test),\\n        \\'f1\\': f1_score(y_test, y_pred_test)\\n    }\\n\\n    # Combine cross-validated and test metrics into a single dictionary\\n    metrics = {**{f\\'cv_{m}\\': cv_metrics[m].mean() for m in cv_metrics}, **test_metrics}\\n\\n    # Step 4: Save the model with timestamp and type\\n    timestamp = datetime.now().strftime(\\'%Y-%m-%d_%H-%M-%S\\')\\n    model_filename = f\\'logistic_regression_{timestamp}.pkl\\'\\n\\n    # Ensure model directory exists\\n    if not os.path.exists(model_dir):\\n        os.makedirs(model_dir)\\n\\n    # Save the trained model\\n    model_path = os.path.join(model_dir, model_filename)\\n    with open(model_path, \\'wb\\') as f_model:\\n        pickle.dump(logistic_model, f_model)\\n\\n    print(f\"Model saved to: {model_path}\")\\n\\n    return metrics, logistic_model'"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Step 6: Function to train logistic regression, evaluate, and save the model\n",
        "def train_logistic_regression_and_save(X_train, y_train, X_test, y_test, model_dir='~/models/'):\n",
        "    \"\"\"Trains, evaluates a logistic regression model, saves the trained model with a timestamp and returns evaluation metrics.\"\"\"\n",
        "\n",
        "    # Step 1: Train logistic regression model with a progress bar\n",
        "    logistic_model = LogisticRegression(solver='saga', max_iter=2000)\n",
        "    with tqdm(total=100, desc=\"Training Logistic Regression\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "        logistic_model.fit(X_train, y_train)\n",
        "        pbar.update(100)\n",
        "\n",
        "    # Check number of iterations\n",
        "    print(f\"Number of iterations: {logistic_model.n_iter_}\")\n",
        "\n",
        "    # Step 2: Evaluate using cross-validation for accuracy, precision, recall, and F1-score\n",
        "    cv_metrics = {}\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        with tqdm(total=5, desc=f\"Cross-Validation ({metric})\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
        "            cv_metrics[metric] = cross_val_score(logistic_model, X_train, y_train, cv=5, scoring=metric)\n",
        "            pbar.update(5)\n",
        "\n",
        "    print(f\"Cross-validated Metrics: {', '.join([f'{m}: {cv_metrics[m].mean():.4f}' for m in cv_metrics])}\")\n",
        "\n",
        "    # Step 3: Test on the test set\n",
        "    y_pred_test = logistic_model.predict(X_test)\n",
        "\n",
        "    # Calculate test set metrics\n",
        "    test_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred_test),\n",
        "        'precision': precision_score(y_test, y_pred_test),\n",
        "        'recall': recall_score(y_test, y_pred_test),\n",
        "        'f1': f1_score(y_test, y_pred_test)\n",
        "    }\n",
        "\n",
        "    # Combine cross-validated and test metrics into a single dictionary\n",
        "    metrics = {**{f'cv_{m}': cv_metrics[m].mean() for m in cv_metrics}, **test_metrics}\n",
        "\n",
        "    # Step 4: Save the model with timestamp and type\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    model_filename = f'logistic_regression_{timestamp}.pkl'\n",
        "\n",
        "    # Ensure model directory exists\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = os.path.join(model_dir, model_filename)\n",
        "    with open(model_path, 'wb') as f_model:\n",
        "        pickle.dump(logistic_model, f_model)\n",
        "\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "    return metrics, logistic_model'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_grid_search(X_train, y_train):\n",
        "    \"\"\"Runs a grid search on logistic regression model to find the best hyperparameters.\"\"\"\n",
        "\n",
        "    # Define the parameter grid for Logistic Regression\n",
        "    param_grid = {\n",
        "        'solver': ['saga', 'lbfgs'],  # Different solvers\n",
        "        'max_iter': [1500, 2000, 2500],  # Number of iterations\n",
        "        'C': [0.005, 0.007, 0.01, 0.12, 0.15]  # Regularization strength\n",
        "    }\n",
        "\n",
        "    # Create a Logistic Regression model\n",
        "    logistic_model = LogisticRegression()\n",
        "\n",
        "    # Set up the GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=logistic_model,\n",
        "        param_grid=param_grid,\n",
        "        scoring='precision',  # Choose appropriate scoring metric\n",
        "        cv=5,  # Number of cross-validation folds\n",
        "        n_jobs=-1,  # Use all available cores\n",
        "        verbose=1  # Verbosity level\n",
        "    )\n",
        "\n",
        "    # Fit Grid Search\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters and best score\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "    print(f\"Best cross-validation score: {best_score:.4f}\")\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    return best_model, best_params, best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnR372rBhduh"
      },
      "source": [
        "# **Running the functions on the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_7Ktzy3hj9u",
        "outputId": "00c8bbe3-1775-467b-ba43-4039b75fd042"
      },
      "outputs": [],
      "source": [
        "\n",
        "# preprocessor import create_target_variable, group_train_test_split, identify_feature_types, create_preprocessing_pipeline,preprocess_training_data, preprocess_new_data,train_logistic_regression, filter_data\n",
        "\n",
        "\n",
        "df = pd.read_csv('~/Small-Cap-Scout/raw_data/data_for_preprocessing.csv', index_col=[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "hl1aO3mihsKZ",
        "outputId": "9d7e5310-9db6-4671-9172-4bd86b40c8f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cik</th>\n",
              "      <th>date</th>\n",
              "      <th>Assets</th>\n",
              "      <th>AssetsCurrent</th>\n",
              "      <th>Cash</th>\n",
              "      <th>AssetsNoncurrent</th>\n",
              "      <th>Liabilities</th>\n",
              "      <th>LiabilitiesCurrent</th>\n",
              "      <th>LiabilitiesNoncurrent</th>\n",
              "      <th>Equity</th>\n",
              "      <th>...</th>\n",
              "      <th>TICKER</th>\n",
              "      <th>market_cap</th>\n",
              "      <th>mc_qtr_growth</th>\n",
              "      <th>mc_qtr_growth_pct</th>\n",
              "      <th>mc_yr_growth</th>\n",
              "      <th>mc_yr_growth_pct</th>\n",
              "      <th>mc_2yr_growth</th>\n",
              "      <th>mc_2yr_growth_pct</th>\n",
              "      <th>small_cap</th>\n",
              "      <th>micro_cap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1750</td>\n",
              "      <td>2011-02-28</td>\n",
              "      <td>1.655991e+09</td>\n",
              "      <td>9.278390e+08</td>\n",
              "      <td>54716000.0</td>\n",
              "      <td>409295000.0</td>\n",
              "      <td>8.513950e+08</td>\n",
              "      <td>419182000.0</td>\n",
              "      <td>432213000.0</td>\n",
              "      <td>804596000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>AIR</td>\n",
              "      <td>1045.889727</td>\n",
              "      <td>46.783392</td>\n",
              "      <td>0.046825</td>\n",
              "      <td>77.281557</td>\n",
              "      <td>0.079786</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1750</td>\n",
              "      <td>2011-05-31</td>\n",
              "      <td>1.703727e+09</td>\n",
              "      <td>9.139850e+08</td>\n",
              "      <td>57433000.0</td>\n",
              "      <td>465365000.0</td>\n",
              "      <td>8.684380e+08</td>\n",
              "      <td>416010000.0</td>\n",
              "      <td>452428000.0</td>\n",
              "      <td>835289000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>AIR</td>\n",
              "      <td>1024.472219</td>\n",
              "      <td>-21.417508</td>\n",
              "      <td>-0.020478</td>\n",
              "      <td>306.796787</td>\n",
              "      <td>0.427487</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1750</td>\n",
              "      <td>2011-08-31</td>\n",
              "      <td>1.752372e+09</td>\n",
              "      <td>9.442470e+08</td>\n",
              "      <td>35523000.0</td>\n",
              "      <td>472856000.0</td>\n",
              "      <td>9.032430e+08</td>\n",
              "      <td>350085000.0</td>\n",
              "      <td>553158000.0</td>\n",
              "      <td>849129000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>AIR</td>\n",
              "      <td>882.619592</td>\n",
              "      <td>-141.852627</td>\n",
              "      <td>-0.138464</td>\n",
              "      <td>255.395538</td>\n",
              "      <td>0.407184</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1750</td>\n",
              "      <td>2011-11-30</td>\n",
              "      <td>1.821612e+09</td>\n",
              "      <td>9.550530e+08</td>\n",
              "      <td>27870000.0</td>\n",
              "      <td>521431000.0</td>\n",
              "      <td>9.582200e+08</td>\n",
              "      <td>374944000.0</td>\n",
              "      <td>583276000.0</td>\n",
              "      <td>863392000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>AIR</td>\n",
              "      <td>727.886752</td>\n",
              "      <td>-154.732840</td>\n",
              "      <td>-0.175311</td>\n",
              "      <td>-271.219583</td>\n",
              "      <td>-0.271462</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1750</td>\n",
              "      <td>2012-02-29</td>\n",
              "      <td>2.220293e+09</td>\n",
              "      <td>1.065389e+09</td>\n",
              "      <td>59294000.0</td>\n",
              "      <td>797765000.0</td>\n",
              "      <td>1.328974e+09</td>\n",
              "      <td>560986000.0</td>\n",
              "      <td>767988000.0</td>\n",
              "      <td>891319000.0</td>\n",
              "      <td>...</td>\n",
              "      <td>AIR</td>\n",
              "      <td>899.522315</td>\n",
              "      <td>171.635564</td>\n",
              "      <td>0.235800</td>\n",
              "      <td>-146.367411</td>\n",
              "      <td>-0.139945</td>\n",
              "      <td>-69.085854</td>\n",
              "      <td>-0.071325</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 59 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    cik        date        Assets  AssetsCurrent        Cash  \\\n",
              "0  1750  2011-02-28  1.655991e+09   9.278390e+08  54716000.0   \n",
              "1  1750  2011-05-31  1.703727e+09   9.139850e+08  57433000.0   \n",
              "2  1750  2011-08-31  1.752372e+09   9.442470e+08  35523000.0   \n",
              "3  1750  2011-11-30  1.821612e+09   9.550530e+08  27870000.0   \n",
              "4  1750  2012-02-29  2.220293e+09   1.065389e+09  59294000.0   \n",
              "\n",
              "   AssetsNoncurrent   Liabilities  LiabilitiesCurrent  LiabilitiesNoncurrent  \\\n",
              "0       409295000.0  8.513950e+08         419182000.0            432213000.0   \n",
              "1       465365000.0  8.684380e+08         416010000.0            452428000.0   \n",
              "2       472856000.0  9.032430e+08         350085000.0            553158000.0   \n",
              "3       521431000.0  9.582200e+08         374944000.0            583276000.0   \n",
              "4       797765000.0  1.328974e+09         560986000.0            767988000.0   \n",
              "\n",
              "        Equity  ...  TICKER   market_cap  mc_qtr_growth  mc_qtr_growth_pct  \\\n",
              "0  804596000.0  ...     AIR  1045.889727      46.783392           0.046825   \n",
              "1  835289000.0  ...     AIR  1024.472219     -21.417508          -0.020478   \n",
              "2  849129000.0  ...     AIR   882.619592    -141.852627          -0.138464   \n",
              "3  863392000.0  ...     AIR   727.886752    -154.732840          -0.175311   \n",
              "4  891319000.0  ...     AIR   899.522315     171.635564           0.235800   \n",
              "\n",
              "   mc_yr_growth  mc_yr_growth_pct  mc_2yr_growth  mc_2yr_growth_pct  \\\n",
              "0     77.281557          0.079786            NaN                NaN   \n",
              "1    306.796787          0.427487            NaN                NaN   \n",
              "2    255.395538          0.407184            NaN                NaN   \n",
              "3   -271.219583         -0.271462            NaN                NaN   \n",
              "4   -146.367411         -0.139945     -69.085854          -0.071325   \n",
              "\n",
              "   small_cap  micro_cap  \n",
              "0          1          0  \n",
              "1          1          0  \n",
              "2          1          0  \n",
              "3          1          0  \n",
              "4          1          0  \n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ren7WhjaxXoH",
        "outputId": "4c2c7da9-4af8-4fe0-ee78-44d18e9f5224"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['cik', 'date', 'Assets', 'AssetsCurrent', 'Cash', 'AssetsNoncurrent',\n",
              "       'Liabilities', 'LiabilitiesCurrent', 'LiabilitiesNoncurrent', 'Equity',\n",
              "       'HolderEquity', 'RetainedEarnings', 'AdditionalPaidInCapital',\n",
              "       'TreasuryStockValue', 'TemporaryEquity', 'RedeemableEquity',\n",
              "       'LiabilitiesAndEquity', 'Revenues', 'CostOfRevenue', 'GrossProfit',\n",
              "       'OperatingExpenses', 'OperatingIncomeLoss',\n",
              "       'IncomeLossFromContinuingOperationsBeforeIncomeTaxExpenseBenefit',\n",
              "       'AllIncomeTaxExpenseBenefit', 'IncomeLossFromContinuingOperations',\n",
              "       'IncomeLossFromDiscontinuedOperationsNetOfTax', 'ProfitLoss',\n",
              "       'NetIncomeLossAttributableToNoncontrollingInterest', 'NetIncomeLoss',\n",
              "       'NetCashProvidedByUsedInOperatingActivitiesContinuingOperations',\n",
              "       'NetCashProvidedByUsedInFinancingActivitiesContinuingOperations',\n",
              "       'NetCashProvidedByUsedInInvestingActivitiesContinuingOperations',\n",
              "       'NetCashProvidedByUsedInOperatingActivities',\n",
              "       'NetCashProvidedByUsedInFinancingActivities',\n",
              "       'NetCashProvidedByUsedInInvestingActivities',\n",
              "       'CashProvidedByUsedInOperatingActivitiesDiscontinuedOperations',\n",
              "       'CashProvidedByUsedInInvestingActivitiesDiscontinuedOperations',\n",
              "       'CashProvidedByUsedInFinancingActivitiesDiscontinuedOperations',\n",
              "       'EffectOfExchangeRateFinal',\n",
              "       'CashPeriodIncreaseDecreaseIncludingExRateEffectFinal', 'afs', 'sic_2d',\n",
              "       'quarter', 'year', 'GDP', 'interest_rate', 'unemployment_rate',\n",
              "       'median_cpi', 'CIK', 'TICKER', 'market_cap', 'mc_qtr_growth',\n",
              "       'mc_qtr_growth_pct', 'mc_yr_growth', 'mc_yr_growth_pct',\n",
              "       'mc_2yr_growth', 'mc_2yr_growth_pct', 'small_cap', 'micro_cap'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "c1P5otIWl2Dl"
      },
      "outputs": [],
      "source": [
        "# Step 2: Drop unwanted columns before target creation\n",
        "df_cleaned = drop_columns(df, cols_to_drop=['cik', 'CIK', 'date', 'stprba', 'quarter', 'year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "fqikqbbIl-TV"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create target variables and split the data\n",
        "df_qtr = create_target_variable(df_cleaned, frequency=1, threshold=0.5)\n",
        "df_yr = create_target_variable(df_cleaned, frequency=4, threshold=0.5)\n",
        "df_2yr = create_target_variable(df_cleaned, frequency=8, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "s22arfsqmA2H"
      },
      "outputs": [],
      "source": [
        "X_train_qtr, X_test_qtr, y_train_qtr, y_test_qtr = group_train_test_split(df_qtr)\n",
        "X_train_yr, X_test_yr, y_train_yr, y_test_yr = group_train_test_split(df_yr)\n",
        "X_train_2yr, X_test_2yr, y_train_2yr, y_test_2yr = group_train_test_split(df_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "VxzH3rREuc4t"
      },
      "outputs": [],
      "source": [
        "# Step 4:  Identify feature types after splitting\n",
        "numerical_features_qtr, categorical_features_qtr = identify_feature_types(X_train_qtr)\n",
        "numerical_features_yr, categorical_features_yr = identify_feature_types(X_train_yr)\n",
        "numerical_features_2yr, categorical_features_2yr = identify_feature_types(X_train_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "abVrvnn6ztXY"
      },
      "outputs": [],
      "source": [
        "# Step 5: Create the preprocessing pipeline\n",
        "preprocessor_qtr = create_preprocessing_pipeline(numerical_features_qtr, categorical_features_qtr)\n",
        "preprocessor_yr = create_preprocessing_pipeline(numerical_features_yr, categorical_features_yr)\n",
        "preprocessor_2yr = create_preprocessing_pipeline(numerical_features_2yr, categorical_features_2yr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "ChuCubny0QXz"
      },
      "outputs": [],
      "source": [
        "# Step 6: Preprocess the training data\n",
        "X_train_qtr_processed, preprocessor_qtr = preprocess_training_data(X_train_qtr, preprocessor=preprocessor_qtr)\n",
        "X_train_yr_processed, preprocessor_yr = preprocess_training_data(X_train_yr, preprocessor=preprocessor_yr)\n",
        "X_train_2yr_processed, preprocessor_2yr = preprocess_training_data(X_train_2yr, preprocessor=preprocessor_2yr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "TBpPlofv0SO6"
      },
      "outputs": [],
      "source": [
        "# Step 7 : then the test data\n",
        "X_test_qtr_processed = preprocess_new_data(X_test_qtr, preprocessor_qtr)\n",
        "X_test_yr_processed = preprocess_new_data(X_test_yr, preprocessor_yr)\n",
        "X_test_2yr_processed = preprocess_new_data(X_test_2yr, preprocessor_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training logistic_regression: 100%|██████████ [elapsed: 00:01 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of iterations: [85]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:14 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:13 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:13 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:13 left: 00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.6612, precision: 0.3373, recall: 0.5328, f1: 0.4066\n",
            "Model saved to: ~/models/logistic_regression_2024-09-10_22-16-24.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 8 :Train for quarterly (frequency=1), yearly (frequency=4), and 2-year (frequency=8) predictions\n",
        "y_pred_qtr_log_reg, model_qtr_log_reg = train_logistic_regression_and_save(X_train_qtr_processed, y_train_qtr, X_test_qtr_processed, y_test_qtr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training logistic_regression: 100%|██████████ [elapsed: 00:01 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of iterations: [88]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:12 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:11 left: 00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.6709, precision: 0.3355, recall: 0.4955, f1: 0.3949\n",
            "Model saved to: ~/models/logistic_regression_2024-09-10_22-18-52.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred_yr_log_reg, model_yr_log_reg = train_logistic_regression_and_save(X_train_yr_processed, y_train_yr, X_test_yr_processed, y_test_yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOMfwVi_1GDf",
        "outputId": "c2be492d-a575-47fc-c9c9-0ee5e3352a01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training logistic_regression: 100%|██████████ [elapsed: 00:06 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of iterations: [648]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:31 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:31 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:32 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:31 left: 00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.6642, precision: 0.3450, recall: 0.5357, f1: 0.4132\n",
            "Model saved to: ~/models/logistic_regression_2024-09-10_22-32-54.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred_2yr_log_reg, model_2yr = train_logistic_regression_and_save(X_train_2yr_processed, y_train_2yr, X_test_2yr_processed, y_test_2yr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training knn: 100%|██████████ [elapsed: 00:00 left: 00:00]\n",
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:12 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:11 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.8230, precision: 0.6547, recall: 0.4248, f1: 0.5129\n",
            "Model saved to: ~/models/knn_2024-09-10_23-03-02.pkl\n"
          ]
        }
      ],
      "source": [
        "y_pred_qtr_knn, model_qtr_knn = train_knn_and_save(X_train_qtr_processed, y_train_qtr, X_test_qtr_processed, y_test_qtr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training knn: 100%|██████████ [elapsed: 00:00 left: 00:00]\n",
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:12 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.8231, precision: 0.6545, recall: 0.4216, f1: 0.5111\n",
            "Model saved to: ~/models/knn_2024-09-10_23-04-45.pkl\n"
          ]
        }
      ],
      "source": [
        "y_pred_yr_knn, model_yr_knn = train_knn_and_save(X_train_yr_processed, y_train_yr, X_test_yr_processed, y_test_yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training knn: 100%|██████████ [elapsed: 00:00 left: 00:00]\n",
            "Cross-Validation (accuracy): 100%|██████████ [elapsed: 00:12 left: 00:00]\n",
            "Cross-Validation (precision): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (recall): 100%|██████████ [elapsed: 00:11 left: 00:00]\n",
            "Cross-Validation (f1): 100%|██████████ [elapsed: 00:11 left: 00:00]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validated Metrics: accuracy: 0.8240, precision: 0.6548, recall: 0.4269, f1: 0.5147\n",
            "Model saved to: ~/models/knn_2024-09-10_23-07-47.pkl\n"
          ]
        }
      ],
      "source": [
        "y_pred_2yr_knn, model_2yr_knn = train_knn_and_save(X_train_2yr_processed, y_train_2yr, X_test_2yr_processed, y_test_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training svc_rbf:   0%|           [elapsed: 00:00 left: ?]"
          ]
        }
      ],
      "source": [
        "y_pred_qtr_svc, model_qtr_svc = train_svc_rbf_and_save(X_train_qtr_processed, y_train_qtr, X_test_qtr_processed, y_test_qtr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_yr_svc, model_yr_svc = train_svc_rbf_and_save(X_train_yr_processed, y_train_yr, X_test_yr_processed, y_test_yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_2yr_svc, model_2yr_svc = train_svc_rbf_and_save(X_train_2yr_processed, y_train_2yr, X_test_2yr_processed, y_test_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7IFuQncCFAOI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Quarter Ahead Metrics: {'cv_accuracy': np.float64(0.5861177525890302), 'cv_precision': np.float64(0.28886531098782686), 'cv_recall': np.float64(0.3746608634582686), 'cv_f1': np.float64(0.32563369780978374), 'accuracy': 0.5739110464671209, 'precision': np.float64(0.28430531732418524), 'recall': np.float64(0.3731007315700619), 'f1': np.float64(0.32270625456315405)}\n",
            "1 Year Ahead Metrics: {'cv_accuracy': np.float64(0.5847497987573081), 'cv_precision': np.float64(0.2876763455297774), 'cv_recall': np.float64(0.37019406810692057), 'cv_f1': np.float64(0.3235231859899204), 'accuracy': 0.6013502387617322, 'precision': np.float64(0.3058765674944677), 'recall': np.float64(0.3812442537542139), 'f1': np.float64(0.33942701227830835)}\n",
            "2 Years Ahead Metrics: {'cv_accuracy': np.float64(0.5855911208007135), 'cv_precision': np.float64(0.2907180862789915), 'cv_recall': np.float64(0.378417465607723), 'cv_f1': np.float64(0.32827805522691694), 'accuracy': 0.6088211708099439, 'precision': np.float64(0.30460509390275275), 'recall': np.float64(0.3524858588865734), 'f1': np.float64(0.32680099365166987)}\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Print metrics for each model\n",
        "print(\"1 Quarter Ahead Metrics:\", y_pred_qtr)\n",
        "print(\"1 Year Ahead Metrics:\", y_pred_yr)\n",
        "print(\"2 Years Ahead Metrics:\", y_pred_2yr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eoingaynard/.pyenv/versions/3.10.6/envs/Small-Cap-Scout/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Let's try a grid search - QUARTER\n",
        "best_model_qtr, best_params_qtr, best_score_qtr = run_grid_search(X_train_qtr_processed, y_train_qtr)\n",
        "\n",
        "print(f\"Best Model - QUARTER: {best_model_qtr.__class__.__name__}\")\n",
        "print(f\"Best Parameters - QUARTER: {best_params_qtr}\")\n",
        "print(f\"Best Score - QUARTER: {best_score_qtr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Best parameters: {'C': 0.15, 'max_iter': 1500, 'solver': 'lbfgs'}\n",
            "Best cross-validation score: 0.3543\n",
            "Best Model - YEAR: LogisticRegression\n",
            "Best Parameters - YEAR: {'C': 0.15, 'max_iter': 1500, 'solver': 'lbfgs'}\n",
            "Best Score - YEAR: 0.3543\n"
          ]
        }
      ],
      "source": [
        "# Let's try a grid search - YEAR\n",
        "best_model_yr, best_params_yr, best_score_yr = run_grid_search(X_train_yr_processed, y_train_yr)\n",
        "\n",
        "print(f\"Best Model - YEAR: {best_model_yr.__class__.__name__}\")\n",
        "print(f\"Best Parameters - YEAR: {best_params_yr}\")\n",
        "print(f\"Best Score - YEAR: {best_score_yr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's try a grid search - 2 YEAR\n",
        "best_model_2yr, best_params_2yr, best_score_2yr = run_grid_search(X_train_2yr_processed, y_train_2yr)\n",
        "\n",
        "print(f\"Best Model - YEAR: {best_model_2yr.__class__.__name__}\")\n",
        "print(f\"Best Parameters - YEAR: {best_params_2yr}\")\n",
        "print(f\"Best Score - YEAR: {best_score_2yr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
