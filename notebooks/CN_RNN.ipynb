{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a3aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ensure that all columns are shown and that colum content is not cut\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_rows', 500) # ensure that all rows are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92280bb0",
   "metadata": {},
   "source": [
    "# Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e323880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : target creation + train_test_split\n",
    "# Creating target variables to automate creation of quarterly, yearly and 2-yearly targets, because well, DON'T REPEAT YOURSELF!\n",
    "def create_target_variable(df, frequency:int, threshold):\n",
    "    if frequency == 1:\n",
    "        col = 'mc_qtr_growth_pct'\n",
    "    if frequency == 4:\n",
    "        col = 'mc_yr_growth_pct'\n",
    "    if frequency == 8:\n",
    "        col = 'mc_2yr_growth_pct'\n",
    "   #else:\n",
    "   #    raise ValueError(\"Invalid frequency. Use 1 (quarterly), 4 (yearly), or 8 (2-year).\")\n",
    "    df[col] = df[col].shift(-frequency)\n",
    "    df.dropna(subset=col, inplace=True)\n",
    "    target_func = lambda x: 1 if ((x[col] > threshold) & (x.small_cap == 1)) else 0\n",
    "    df['target'] = df.apply(target_func, axis=1)\n",
    "    return df\n",
    "\n",
    "# Creating a custom function for the group split\n",
    "def group_train_test_split(data, test_size=0.2, random_state=None):\n",
    "    # We split by groups (company ticker) while keeping the data structure intact.\n",
    "    unique_groups = data['Ticker'].unique()\n",
    "    train_groups, test_groups = train_test_split(unique_groups, test_size=test_size, random_state=random_state)\n",
    "    X_train = data[data['Ticker'].isin(train_groups)].drop(['mc_qtr_growth', 'mc_qtr_growth_pct', 'mc_yr_growth', 'mc_yr_growth_pct', 'mc_2yr_growth', 'mc_2yr_growth_pct'], axis = 1)\n",
    "    X_test = data[data['Ticker'].isin(test_groups)].drop(['mc_qtr_growth', 'mc_qtr_growth_pct', 'mc_yr_growth', 'mc_yr_growth_pct', 'mc_2yr_growth', 'mc_2yr_growth_pct'], axis = 1)\n",
    "    y_train = data[data['Ticker'].isin(train_groups)]['target']\n",
    "    y_test = data[data['Ticker'].isin(test_groups)]['target']\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "858b0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Identify numerical and categorical features\n",
    "def identify_feature_types(df):\n",
    "    \"\"\"Identifies the numerical and categorical columns in the DataFrame.\"\"\"\n",
    "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Exclude 'Ticker' from categorical features as it's not needed for transformation\n",
    "    if 'Ticker' in categorical_features:\n",
    "        categorical_features.remove('Ticker')\n",
    "\n",
    "    return numerical_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "afb38594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create preprocessing pipeline for numerical and categorical features\n",
    "def create_preprocessing_pipeline(numerical_features, categorical_features):\n",
    "    \"\"\"Creates the preprocessing pipeline for numerical and categorical features.\"\"\"\n",
    "    # Preprocessing for numerical data: RobustScaler to make our numbers m√°s robusto.\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),  # Handle NaNs\n",
    "        ('scaler', RobustScaler())  # Scale the data\n",
    "    ])\n",
    "\n",
    "    # Preprocessing for categorical data: OneHotEncoder to give each category its own columm...\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categories\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Encode categories\n",
    "    ])\n",
    "\n",
    "    # Combine the transformers into one big ColumnTransformer.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1248d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Function to preprocess data in training mode (fitting the pipeline)\n",
    "def preprocess_training_data(X_train, preprocessor=None):\n",
    "    \"\"\"Fits and transforms the training data using the provided pipeline.\"\"\"\n",
    "    if preprocessor is None:\n",
    "        # Identify feature types\n",
    "        numerical_features, categorical_features = identify_feature_types(X_train)\n",
    "        preprocessor = create_preprocessing_pipeline(numerical_features, categorical_features)\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    return X_train_processed, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "03b0f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Function to preprocess new/unseen/test data in production mode (only transforming)\n",
    "def preprocess_new_data(X_new, preprocessor):\n",
    "    \"\"\"Transforms new/unseen/test data using a pre-fitted pipeline.\"\"\"\n",
    "    if preprocessor is None:\n",
    "        raise ValueError(\"The preprocessor must be fitted on training data first before transforming new data.\")\n",
    "\n",
    "    # Transform the new data (no fitting here)\n",
    "    X_new_processed = preprocessor.transform(X_new)\n",
    "    return X_new_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a1d97174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Function to predict based on different target inputs defined at the create_target_variable stage: quarterly (frequency=1), yearly (frequency=4), and 2-year (frequency=8) predictions\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Trains and evaluates a logistic regression model, and returns multiple evaluation metrics\n",
    "    (accuracy, precision, recall, F1-score) using cross-validation and test data.\n",
    "    \"\"\"\n",
    "    # Train logistic regression model with a progress bar\n",
    "    logistic_model = LogisticRegression(solver='saga', max_iter=5000)\n",
    "\n",
    "    # Display progress during model fitting\n",
    "    with tqdm(total=100, desc=\"Training Logistic Regression\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
    "        logistic_model.fit(X_train, y_train)\n",
    "        pbar.update(100)  # Update the progress bar after training completes\n",
    "\n",
    "    # Check number of iterations\n",
    "    print(f\"Number of iterations: {logistic_model.n_iter_}\")\n",
    "\n",
    "    # Evaluate using cross-validation for accuracy, precision, recall, and F1-score with progress\n",
    "    cv_metrics = {}\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        with tqdm(total=5, desc=f\"Cross-Validation ({metric})\", bar_format='{l_bar}{bar} [elapsed: {elapsed} left: {remaining}]') as pbar:\n",
    "            cv_metrics[metric] = cross_val_score(logistic_model, X_train, y_train, cv=5, scoring=metric)\n",
    "            pbar.update(5)\n",
    "\n",
    "    # Print cross-validation scores\n",
    "    print(f\"Cross-validated Metrics: {', '.join([f'{m}: {cv_metrics[m].mean():.4f}' for m in cv_metrics])}\")\n",
    "\n",
    "    # Test on the test set\n",
    "    y_pred_test = logistic_model.predict(X_test)\n",
    "\n",
    "    # Calculate test set metrics\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'precision': precision_score(y_test, y_pred_test),\n",
    "        'recall': recall_score(y_test, y_pred_test),\n",
    "        'f1': f1_score(y_test, y_pred_test)\n",
    "    }\n",
    "\n",
    "    # Create a dictionary to store all metrics\n",
    "    metrics = {**{f'cv_{m}': cv_metrics[m].mean() for m in cv_metrics}, **test_metrics}\n",
    "\n",
    "    return metrics, logistic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91776f16",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06fa45d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170112, 61)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "df = pd.read_csv('../raw_data/Datasets/merged_data_prelim_no_stocks.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89b33f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_groups = df['TICKER'].unique()\n",
    "train_groups, test_groups = train_test_split(unique_groups, test_size=0.3, random_state=42)\n",
    "\n",
    "data_train = df[df['TICKER'].isin(train_groups)]\n",
    "data_test = df[df['TICKER'].isin(test_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d539284",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>mc_yr_growth_pct</th>\n",
       "      <th>small_cap</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>4</th>\n",
       "      <td>1750</td>\n",
       "      <td>AIR</td>\n",
       "      <td>-0.139945</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1750</td>\n",
       "      <td>AIR</td>\n",
       "      <td>-0.545398</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1750</td>\n",
       "      <td>AIR</td>\n",
       "      <td>-0.321041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1750</td>\n",
       "      <td>AIR</td>\n",
       "      <td>-0.157174</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1750</td>\n",
       "      <td>AIR</td>\n",
       "      <td>-0.233756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CIK TICKER  mc_yr_growth_pct  small_cap  target\n",
       "0 4  1750    AIR         -0.139945          1       0\n",
       "  5  1750    AIR         -0.545398          1       0\n",
       "  6  1750    AIR         -0.321041          1       0\n",
       "  7  1750    AIR         -0.157174          1       0\n",
       "  8  1750    AIR         -0.233756          1       0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = df.iloc[:, -11:]\n",
    "\n",
    "# Year-ahead\n",
    "target_func = lambda x: 1 if ((x['mc_yr_growth_pct'] > 0.5) & (x.small_cap == 1)) else 0\n",
    "y_tar = y_set.groupby(['CIK', 'TICKER'], as_index=False)[['CIK', 'TICKER', 'mc_yr_growth_pct', 'small_cap']].apply(lambda group: group.iloc[4:, :])\n",
    "y_tar['target'] = y_tar.apply(target_func, axis=1)\n",
    "y_tar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9eb93c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = y_set.columns.tolist()\n",
    "cols_drop.remove('TICKER')\n",
    "cols_drop += ['cik', 'date', 'quarter', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d76d45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[data_train['TICKER'].isin(train_groups)].drop(columns=cols_drop).reset_index(drop=True).set_index('TICKER')\n",
    "X_test = data_test[data_test['TICKER'].isin(test_groups)].drop(columns=cols_drop).reset_index(drop=True).set_index('TICKER')\n",
    "y_train = y_tar[y_tar['TICKER'].isin(train_groups)]['target'].reset_index(drop=True)\n",
    "y_test = y_tar[y_tar['TICKER'].isin(test_groups)]['target'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdc0c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape, X_train.cik.nunique() * 4 +y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "35049356",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_test.shape, X_test.cik.nunique() * 4 +y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "578210d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118600, 170)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(51512, 170)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess X_train and X_test\n",
    "num, cat = identify_feature_types(X_train)\n",
    "preprocessor = create_preprocessing_pipeline(num, cat)\n",
    "X_train_pp, preprocessor = preprocess_training_data(X_train, preprocessor=preprocessor)\n",
    "X_test_pp = preprocess_new_data(X_test, preprocessor=preprocessor)\n",
    "display(X_train_pp.shape, X_test_pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7be21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5063b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad53d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- RNN Architecture\n",
    "model = Sequential()\n",
    "model.add(layers.SimpleRNN(units=2, activation='tanh', input_shape=(4,3)))\n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "# 2- Compilation\n",
    "model.compile(loss='mse', \n",
    "              optimizer=Adam(lr=0.5)) # very high lr so we can converge with such a small dataset\n",
    "\n",
    "# 3- Fit\n",
    "model.fit(X, y, epochs=10000, verbose=0)\n",
    "\n",
    "# 4- Predict\n",
    "model.predict(X) # One prediction per city"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
